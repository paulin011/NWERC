\section{Miscellaneous Tips}

\subsection{Python-Specific Optimizations}
\begin{lstlisting}
# Fast input for large datasets
import sys
input = sys.stdin.readline

# Increase recursion limit for deep DFS/DP
sys.setrecursionlimit(10**6)

# Threading for higher stack limit (CAUTION: use carefully)
import threading
threading.stack_size(2**26)  # 64MB
sys.setrecursionlimit(2**20)

# Deep copy (be careful with performance)
from copy import deepcopy
new_list = deepcopy(old_list)

# Fast output (for printing large results)
import sys
print = sys.stdout.write  # Only use for string output
\end{lstlisting}

\subsection{Useful Libraries}
\begin{lstlisting}
# Iterator tools - powerful combinations
from itertools import *

# permutations(iterable, r) - all r-length permutations
perms = list(permutations([1,2,3], 2))
# [(1,2), (1,3), (2,1), (2,3), (3,1), (3,2)]

# combinations(iterable, r) - r-length combinations
combs = list(combinations([1,2,3], 2))
# [(1,2), (1,3), (2,3)]

# product - cartesian product
prod = list(product([1,2], ['a','b']))
# [(1,'a'), (1,'b'), (2,'a'), (2,'b')]

# accumulate - running totals
acc = list(accumulate([1,2,3,4]))
# [1, 3, 6, 10]

# chain - flatten iterables
chained = list(chain([1,2], [3,4]))
# [1, 2, 3, 4]
\end{lstlisting}

\subsection{Common Patterns}
\begin{lstlisting}
# Lambda sorting with multiple keys
arr.sort(key=lambda x: (-x[0], x[1]))
# Sort by first desc, then second asc

# All/Any - short-circuit evaluation
all(x > 0 for x in arr)  # True if all positive
any(x > 0 for x in arr)  # True if any positive

# Zip - parallel iteration
for a, b in zip(list1, list2):
    pass

# Enumerate - index and value
for i, val in enumerate(arr):
    print(f"arr[{i}] = {val}")

# Custom comparison function
from functools import cmp_to_key

def compare(a, b):
    # Return -1 if a < b, 0 if equal, 1 if a > b
    if a + b > b + a:
        return -1
    return 1

arr.sort(key=cmp_to_key(compare))

# DefaultDict with lambda
from collections import defaultdict
d = defaultdict(lambda: float('inf'))

# Multiple assignment
a, b = b, a  # Swap
a, *rest, b = [1,2,3,4,5]  # a=1, rest=[2,3,4], b=5
\end{lstlisting}

\subsection{Common Pitfalls}
\begin{lstlisting}
# Integer division - floors toward negative infinity
print(7 // 3)    # 2
print(-7 // 3)   # -3 (not -2!)

# For ceiling division toward zero:
def div_ceil(a, b):
    return -(-a // b)

# Modulo with negative numbers
print((-5) % 3)  # 1 (not -2!)
print(5 % -3)    # -1

# List multiplication creates references!
matrix = [[0] * m] * n  # WRONG! All rows same object
matrix[0][0] = 1        # Changes all rows!

# Correct way
matrix = [[0] * m for _ in range(n)]

# Float comparison - don't use ==
a, b = 0.1 + 0.2, 0.3
print(a == b)  # False!

# Use epsilon comparison
eps = 1e-9
print(abs(a - b) < eps)  # True

# String immutability
s = "abc"
# s[0] = 'd'  # ERROR!
s = 'd' + s[1:]  # OK

# For many string mutations, use list
chars = list(s)
chars[0] = 'd'
s = ''.join(chars)

# Mutable default arguments - dangerous!
def func(arr=[]):  # WRONG!
    arr.append(1)
    return arr

# Each call modifies same list
print(func())  # [1]
print(func())  # [1, 1]

# Correct way
def func(arr=None):
    if arr is None:
        arr = []
    arr.append(1)
    return arr

# Generator expressions save memory
sum(x*x for x in range(10**6))  # Memory efficient
# vs
sum([x*x for x in range(10**6)])  # Creates full list

# Ternary operator
x = a if condition else b

# Dictionary get with default
count = d.get(key, 0) + 1

# Matrix rotation 90 degrees clockwise
def rotate_90(matrix):
    return [list(row) for row in zip(*matrix[::-1])]

# Matrix transpose
def transpose(matrix):
    return [list(row) for row in zip(*matrix)]
\end{lstlisting}

\subsection{Time Complexity Reference}
\begin{lstlisting}
# Common time complexities (Python, rough guides for 1--2s limits):
# O(1), O(log n): instant
# O(n): usually fine up to ~10^7 operations (~1s)
# O(n log n): OK for n up to several 10^5 depending on constants
# O(n sqrt(n)): risky in Python (may be OK for n up to a few 10^4 with low constants)
# O(n^2): often TLE for n > 10^4
# O(2^n): TLE for n > 20 (unless heavy pruning/memoization)
# O(n!): TLE for n > 11

# Input size guidelines (Python-focused):
# n <= 12: O(n!) (brute-force permutations)
# n <= 20: O(2^n) (subset DP / bitmask DP with n up to ~20)
# n <= 500: O(n^3) may sometimes pass for small constants
# n <= 5000: O(n^2) borderline; optimize heavily
# n <= 10^6: O(n log n) common; O(n) preferred when possible
# n <= 10^7: O(n) may be ok for tight loops
# n > 10^7: aim for O(n) with very low constants, or O(log n)/O(1)
\end{lstlisting}

\textbf{Complexity examples (Python implementations)}

\begin{description}
  \item[$O(1)$] array access, dictionary lookup, push/pop from list end.
  \item[$O(\log n)$] binary search (bisect), heap push/pop (heapq), operations in sortedcontainers.
  \item[$O(n)$] single-pass scans, two-pointers, prefix sums, counting frequencies (Counter).
  \item[$O(n\log n)$] sorting (Timsort via sorted()/list.sort()), heap construction, divide-and-conquer merges.
  \item[$O(n\sqrt{n})$] sqrt-decomposition queries, some Mo's algorithm variants (constant-sensitive).
  \item[$O(n^2)$] nested loops for pairwise checks, naive DP on pairs (be cautious for $n>10{,}000$).
  \item[$O(n^3)$] triple loops (Floyd--Warshall), usually too slow unless $n\le 200$.
  \item[$O(2^n)$] bitmask DP, subset enumerations, recursion over subsets (recommended for $n\le 20$).
  \item[$O(n!)$] full permutations, exhaustive search over orderings (recommended for $n\le 10$; occasionally up to 11).
\end{description}

\textbf{How to use:} This quick reference maps input size $n$ (left) to typical feasible time complexities (right) for contest time limits (1--2s) targeting Python implementations. Use it to pick algorithmic approaches and to decide when to optimize or change strategy.

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{|l|l|}
\hline
\textbf{Input size $n$} & \textbf{Typical feasible complexities (examples in Python)} \\ \hline
$n \le 10$    & $O(n!)$, $O(n\cdot 2^n)$ (full permutations, brute-force orderings) \\ \hline
$n \le 20$    & $O(2^n)$, $O(n\cdot 2^n)$ (bitmask DP, subset enumeration) \\ \hline
$n \le 100$   & $O(n^4)$ (very small), $O(n^3)$ (cubic) for tiny inputs or optimized inner loops \\ \hline
$n \le 500$   & $O(n^3)$ borderline, $O(n^2\log n)$ sometimes OK with low constants \\ \hline
$n \le 2000$  & $O(n^2)$ (tight but common), $O(n\log n)$ safe \\ \hline
$n \le 10^4$  & $O(n^2)$ (possibly optimized), $O(n\log n)$ recommended \\ \hline
$n \le 10^5$  & $O(n\log n)$, $O(n)$ (linear or near-linear algorithms) \\ \hline
$n \le 10^6$  & $O(n)$ or $O(n\log n)$ (careful with constants; prefer linear) \\ \hline
$n > 10^6$     & $O(n)$ (very tight), $O(\log n)$ or $O(1)$ preferred \\ \hline
\end{tabular}
\end{table}

\textbf{Notes on filling the table:}
\begin{itemize}
  \item Start by checking the problem's time limit and target language. These guidelines are Python-focused (assume roughly $\approx 10^{7}$ simple operations/s; actual throughput depends on implementation details and input shapes).
  \item Convert algorithm cost to operation count: roughly cost = $c\cdot f(n)$. If cost $>$ time\_limit $\times$ ops\_per\_sec, it will TLE.
  \item When in doubt, aim one complexity class lower (e.g. prefer $O(n\log n)$ over $O(n^2)$ for $n$ around $10^5$).
  \item Consider memory limits---some faster algorithms use more memory (e.g. segment trees vs. Fenwick tree).
  \item For multivariate inputs, replace $n$ with the product/dominant parameter (e.g. $n\cdot m$) and apply the same rules.
  \item If an algorithm theoretically fits but is close to the limit, try to reduce constant factors: use local variables, avoid heavy Python objects in inner loops, use built-in functions, or move hot code to PyPy/Cython if allowed.
\end{itemize}
